{
 "cells": [
  {
   "cell_type": "code",
   "id": "67d91f7c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T04:15:11.363073Z",
     "start_time": "2025-10-23T04:15:11.356330Z"
    }
   },
   "source": [
    "import os, getpass\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "b94d1a90-2fe3-4b2a-a901-3bdb89e37edc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T04:15:11.821301Z",
     "start_time": "2025-10-23T04:15:11.370370Z"
    }
   },
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage, ToolMessage\n",
    "from langgraph.graph import START, StateGraph, MessagesState\n",
    "from langgraph.prebuilt import tools_condition, ToolNode\n",
    "from IPython.display import Image, display\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "def check_tyre_wear(lap_number: int) -> str:\n",
    "    \"\"\"Checks the estimated tyre wear based on the current lap number.\"\"\"\n",
    "    if lap_number < 15:\n",
    "        return f\"Lap {lap_number}: Tyre wear is low. Estimated 85% life remaining.\"\n",
    "    elif lap_number < 30:\n",
    "        return f\"Lap {lap_number}: Tyre wear is medium. Estimated 50% life remaining.\"\n",
    "    else:\n",
    "        return f\"Lap {lap_number}: Tyre wear is high. Estimated 20% life remaining. Consider pitting.\"\n",
    "\n",
    "def get_weather_forecast(sector: int) -> str:\n",
    "    \"\"\"Gets the weather forecast for a specific sector of the track.\"\"\"\n",
    "    if sector == 1:\n",
    "        return f\"Sector {sector}: Forecast is clear, track temperature stable.\"\n",
    "    elif sector == 2:\n",
    "        return f\"Sector {sector}: Slight chance of light drizzle in the next 10 laps.\"\n",
    "    else:\n",
    "        return f\"Sector {sector}: Track remains dry for now.\"\n",
    "\n",
    "def suggest_pit_strategy(current_lap: int, laps_remaining: int) -> str:\n",
    "    \"\"\"Suggests a pit stop strategy based on current lap and laps remaining.\"\"\"\n",
    "    optimal_pit_window_start = 20\n",
    "    optimal_pit_window_end = 35\n",
    "    if optimal_pit_window_start <= current_lap <= optimal_pit_window_end:\n",
    "        return f\"Lap {current_lap}/{laps_remaining} laps remaining: Currently within the optimal pit window. Evaluate traffic.\"\n",
    "    elif current_lap < optimal_pit_window_start:\n",
    "        return f\"Lap {current_lap}/{laps_remaining} laps remaining: Too early for optimal one-stop. Hold position.\"\n",
    "    else:\n",
    "        return f\"Lap {current_lap}/{laps_remaining} laps remaining: Past optimal window. Consider Plan B if tyres degrade rapidly.\"\n",
    "\n",
    "\n",
    "tools = [check_tyre_wear, get_weather_forecast, suggest_pit_strategy]\n",
    "llm = ChatOpenAI(model=\"gpt-4.1-nano\")\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "sys_msg = SystemMessage(content=\"You are a helpful F1 race engineer providing concise updates and strategy advice to the driver based on available tools.\")\n",
    "\n",
    "def race_engineer_assistant(state: MessagesState):\n",
    "   return {\"messages\": [llm_with_tools.invoke([sys_msg] + state[\"messages\"])]}\n",
    "\n",
    "builder = StateGraph(MessagesState)\n",
    "\n",
    "builder.add_node(\"assistant\", race_engineer_assistant)\n",
    "builder.add_node(\"tools\", ToolNode(tools))\n",
    "\n",
    "builder.add_edge(START, \"assistant\")\n",
    "builder.add_conditional_edges(\n",
    "    \"assistant\",\n",
    "    tools_condition,\n",
    ")\n",
    "builder.add_edge(\"tools\", \"assistant\")\n",
    "\n",
    "memory = MemorySaver()\n",
    "graph = builder.compile(interrupt_before=[\"tools\"], checkpointer=memory)\n",
    "\n",
    "display(Image(graph.get_graph(xray=True).draw_mermaid_png()))"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "a783efac-46a9-4fb4-a1c6-a11b02540448",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T04:15:14.002635Z",
     "start_time": "2025-10-23T04:15:12.201205Z"
    }
   },
   "source": [
    "initial_input = {\"messages\": [HumanMessage(content=\"What's the tyre wear like on lap 25?\")]}\n",
    "thread = {\"configurable\": {\"thread_id\": \"f1_thread_1\"}}\n",
    "\n",
    "for event in graph.stream(initial_input, thread, stream_mode=\"values\"):\n",
    "    if \"messages\" in event:\n",
    "      event['messages'][-1].pretty_print()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001B[1m Human Message \u001B[0m=================================\n",
      "\n",
      "Multiply 2 and 3\n",
      "==================================\u001B[1m Ai Message \u001B[0m==================================\n",
      "Tool Calls:\n",
      "  multiply (call_AYjV7DyVpP1cd5seOxT6CeDA)\n",
      " Call ID: call_AYjV7DyVpP1cd5seOxT6CeDA\n",
      "  Args:\n",
      "    a: 2\n",
      "    b: 3\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "61569596-8342-4a37-9c99-e3a9dccb18ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T04:15:14.010879Z",
     "start_time": "2025-10-23T04:15:14.007383Z"
    }
   },
   "source": [
    "state = graph.get_state(thread)\n",
    "state.next"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('tools',)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "896a5f41-7386-4bfa-a78e-3e6ca5e26641",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-10-23T04:15:14.014044Z"
    }
   },
   "source": [
    "for event in graph.stream(None, thread, stream_mode=\"values\"):\n",
    "    if \"messages\" in event:\n",
    "      event['messages'][-1].pretty_print()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001B[1m Ai Message \u001B[0m==================================\n",
      "Tool Calls:\n",
      "  multiply (call_AYjV7DyVpP1cd5seOxT6CeDA)\n",
      " Call ID: call_AYjV7DyVpP1cd5seOxT6CeDA\n",
      "  Args:\n",
      "    a: 2\n",
      "    b: 3\n",
      "=================================\u001B[1m Tool Message \u001B[0m=================================\n",
      "Name: multiply\n",
      "\n",
      "6\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95a0eb50-66e3-4538-8103-207aae175154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001B[1m Human Message \u001B[0m=================================\n",
      "\n",
      "Multiply 2 and 3\n",
      "==================================\u001B[1m Ai Message \u001B[0m==================================\n",
      "Tool Calls:\n",
      "  multiply (call_tpHvTmsHSjSpYnymzdx553SU)\n",
      " Call ID: call_tpHvTmsHSjSpYnymzdx553SU\n",
      "  Args:\n",
      "    a: 2\n",
      "    b: 3\n",
      "==================================\u001B[1m Ai Message \u001B[0m==================================\n",
      "Tool Calls:\n",
      "  multiply (call_tpHvTmsHSjSpYnymzdx553SU)\n",
      " Call ID: call_tpHvTmsHSjSpYnymzdx553SU\n",
      "  Args:\n",
      "    a: 2\n",
      "    b: 3\n",
      "=================================\u001B[1m Tool Message \u001B[0m=================================\n",
      "Name: multiply\n",
      "\n",
      "6\n",
      "==================================\u001B[1m Ai Message \u001B[0m==================================\n",
      "\n",
      "The result of multiplying 2 and 3 is 6.\n"
     ]
    }
   ],
   "source": [
    "initial_input = {\"messages\": [HumanMessage(content=\"Any rain expected in sector 2?\")]}\n",
    "thread = {\"configurable\": {\"thread_id\": \"f1_thread_2\"}}\n",
    "\n",
    "for event in graph.stream(initial_input, thread, stream_mode=\"values\"):\n",
    "    if \"messages\" in event:\n",
    "      event['messages'][-1].pretty_print()\n",
    "\n",
    "user_approval = input(\"Approve tool call? (yes/no): \")\n",
    "\n",
    "if user_approval.lower() == \"yes\":\n",
    "    for event in graph.stream(None, thread, stream_mode=\"values\"):\n",
    "         if \"messages\" in event:\n",
    "            event['messages'][-1].pretty_print()\n",
    "else:\n",
    "    print(\"Tool call cancelled by user.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63c2eaf1-6b8b-4d80-9902-98ae5587bcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'google.colab' in str(get_ipython()):\n",
    "    raise Exception(\"Unfortunately LangGraph Studio is currently not supported on Google Colab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb1dd890-c216-4802-9e33-b637e491e144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the URL of the local development server\n",
    "from langgraph_sdk import get_client\n",
    "client = get_client(url=\"http://127.0.0.1:2024\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de9c5017-3a15-46f6-8edf-3997613da323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving new event of type: metadata...\n",
      "--------------------------------------------------\n",
      "Receiving new event of type: values...\n",
      "{'content': 'Multiply 2 and 3', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': '2a3b1e7a-f6d9-44c2-a4b4-b7f67aa3691c', 'example': False}\n",
      "--------------------------------------------------\n",
      "Receiving new event of type: values...\n",
      "{'content': '', 'additional_kwargs': {'tool_calls': [{'id': 'call_ElnkVOf1H80dlwZLqO0PQTwS', 'function': {'arguments': '{\"a\":2,\"b\":3}', 'name': 'multiply'}, 'type': 'function'}], 'refusal': None}, 'response_metadata': {'token_usage': {'completion_tokens': 18, 'prompt_tokens': 134, 'total_tokens': 152, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_eb9dce56a8', 'finish_reason': 'tool_calls', 'logprobs': None}, 'type': 'ai', 'name': None, 'id': 'run-89ee14dc-5f46-4dd9-91d9-e922c4a23572-0', 'example': False, 'tool_calls': [{'name': 'multiply', 'args': {'a': 2, 'b': 3}, 'id': 'call_ElnkVOf1H80dlwZLqO0PQTwS', 'type': 'tool_call'}], 'invalid_tool_calls': [], 'usage_metadata': {'input_tokens': 134, 'output_tokens': 18, 'total_tokens': 152, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}}\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "initial_input = {\"messages\": [HumanMessage(content=\"Suggest a pit strategy. Current lap 22, 35 laps remaining.\")]}\n",
    "thread = await client.threads.create()\n",
    "async for chunk in client.runs.stream(\n",
    "    thread[\"thread_id\"],\n",
    "    assistant_id=\"f1_race_data\", # Make sure this matches your langgraph.json key\n",
    "    input=initial_input,\n",
    "    stream_mode=\"values\",\n",
    "    interrupt_before=[\"tools\"],\n",
    "):\n",
    "    print(f\"Receiving new event of type: {chunk.event}...\")\n",
    "    messages = chunk.data.get('messages', [])\n",
    "    if messages:\n",
    "        print(messages[-1])\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76284730-9c90-46c4-8295-400a49760b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving new event of type: metadata...\n",
      "--------------------------------------------------\n",
      "Receiving new event of type: values...\n",
      "{'content': '', 'additional_kwargs': {'tool_calls': [{'id': 'call_ElnkVOf1H80dlwZLqO0PQTwS', 'function': {'arguments': '{\"a\":2,\"b\":3}', 'name': 'multiply'}, 'type': 'function'}], 'refusal': None}, 'response_metadata': {'token_usage': {'completion_tokens': 18, 'prompt_tokens': 134, 'total_tokens': 152, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_eb9dce56a8', 'finish_reason': 'tool_calls', 'logprobs': None}, 'type': 'ai', 'name': None, 'id': 'run-89ee14dc-5f46-4dd9-91d9-e922c4a23572-0', 'example': False, 'tool_calls': [{'name': 'multiply', 'args': {'a': 2, 'b': 3}, 'id': 'call_ElnkVOf1H80dlwZLqO0PQTwS', 'type': 'tool_call'}], 'invalid_tool_calls': [], 'usage_metadata': {'input_tokens': 134, 'output_tokens': 18, 'total_tokens': 152, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}}\n",
      "--------------------------------------------------\n",
      "Receiving new event of type: values...\n",
      "{'content': '6', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': 'multiply', 'id': '5331919f-a26b-4d75-bf33-6dfaea2be1f7', 'tool_call_id': 'call_ElnkVOf1H80dlwZLqO0PQTwS', 'artifact': None, 'status': 'success'}\n",
      "--------------------------------------------------\n",
      "Receiving new event of type: values...\n",
      "{'content': 'The result of multiplying 2 and 3 is 6.', 'additional_kwargs': {'refusal': None}, 'response_metadata': {'token_usage': {'completion_tokens': 15, 'prompt_tokens': 159, 'total_tokens': 174, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_eb9dce56a8', 'finish_reason': 'stop', 'logprobs': None}, 'type': 'ai', 'name': None, 'id': 'run-06b901ad-0760-4986-9d3f-a566e0d52efd-0', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': {'input_tokens': 159, 'output_tokens': 15, 'total_tokens': 174, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}}\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "async for chunk in client.runs.stream(\n",
    "    thread[\"thread_id\"],\n",
    "    \"f1_race_data\", # Make sure this matches your langgraph.json key\n",
    "    input=None,\n",
    "    stream_mode=\"values\",\n",
    "    interrupt_before=[\"tools\"], # Can keep or remove interrupt for continuation\n",
    "):\n",
    "    print(f\"Receiving new event of type: {chunk.event}...\")\n",
    "    messages = chunk.data.get('messages', [])\n",
    "    if messages:\n",
    "        print(messages[-1])\n",
    "    print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "academy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
