# nithurshen-langgraph-MAT496
This repository documents my learning from the LangChain Academy's "Intro to LangGraph" course. Each commit corresponds to a specific video lesson and features personalized modifications to the original source code. The README file serves as a detailed log, containing a summary of key concepts from each video and notes on my code tweaks.

<b>Name:</b> K S Nithurshen  
<b>Roll No:</b> 2410110157  

## Module 0:
I've learned the fundamentals of setting up a LangChain environment and interacting with its core components. The course covered how to instantiate different ChatOpenAI models, like gpt-4o and gpt-3.5-turbo, and how to use the .invoke() method to send prompts. It also introduced the integration of external search tools, specifically using TavilySearchResults. I've put these concepts into practice in my own notebook by experimenting with other models like gpt-4.1-nano and gpt-4o-mini. To align the exercise with my interest in Formula 1, I modified the tool run by changing the TavilySearchResults query to "Who is Max Verstappen?" and increased the max_results parameter to 50 for a more extensive search. (https://github.com/Nithurshen/nithurshen-langgraph-MAT496/blob/main/module-0/my_example_of_basics.ipynb)

## Module 1: (Introduction)
* <b>Lesson 1:</b> I've learned about the fundamental trade-off in building LLM applications between the reliability of fixed, linear "chains" and the agency or flexibility of "agents". While chains are dependable because they execute the same control flow every time, they are limited. Agents are more powerful because they use an LLM to decide their own control flow, but this makes them less predictable. LangGraph is presented as the solution to this problem, designed to optimize the curve by enabling developers to build applications as graphs. This graph-based approach allows for a balance of reliability and control by letting the developer define the structure while injecting an LLM at specific points to make routing decisions. These graphs operate on a shared state or memory, and nodes can call tools to modify that state, providing a robust framework for creating more dependable and controllable agents.
* <b>Lesson 2:</b> Based on this lesson, I've learned how to build a simple application using LangGraph's core components. The process starts with defining a State using a TypedDict, which acts as a shared memory schema for the entire graph. I then created Nodes, which are Python functions that take this state as an argument and return a dictionary to update it. The key to creating dynamic workflows is using Conditional Edges, which are functions that route the execution to different nodes based on the current state. I put this into practice by building a more complex, F1-themed graph than the one in the lesson. My implementation uses eight nodes and three separate conditional edges to randomly generate a sentence about a race, deciding on the driver (e.g., Max Verstappen or Sebastian Vettel), the outcome (won or lost), and the specific event (Monaco GP or Le Mans). Finally, I compiled the graph and successfully used the .invoke() method to run it, confirming my understanding of how to construct and execute a stateful, branching workflow. (https://github.com/Nithurshen/nithurshen-langgraph-MAT496/blob/main/module-1/lesson_2/my_example_of_simple_graph.ipynb)

![l2_m1](https://github.com/Nithurshen/nithurshen-langgraph-MAT496/blob/main/module-1/lesson_2/l2_m1.png)
* <b>Lesson 3:</b> From this lesson, I've learned how LangSmith and the new LangGraph Studio are essential tools for debugging, visualizing, and iterating on LangGraph applications. The video explained that every time a graph, like the F1-themed one I wrote in f1_exmple.py, is run, it creates a detailed trace in LangSmith. As shown in the screenshot of my trace, this is incredibly powerful for debugging because it provides a step-by-step log of the execution flow and shows exactly how the Graph State changed as it moved through the nodes, from node_1 to node_2 ("I am Max Verstappen"), and eventually to node_8. The lesson also highlighted how the LangGraph Studio acts as a visual IDE. The left panel in my screenshot shows the complete structure of my graph, which was generated from my Python code, making it easy to understand the branching logic I implemented and how the final state was reached. (https://github.com/Nithurshen/nithurshen-langgraph-MAT496/blob/main/f1_exmple.py)

![l3_m1](https://github.com/Nithurshen/nithurshen-langgraph-MAT496/blob/main/module-1/lesson_3/l3_m1.png)
* <b>Lesson 4:</b> I've learned how to build a foundational LangGraph "chain" that can intelligently decide when to use external tools. A key concept was managing conversational history by using the pre-built MessagesState for the graph's state, which leverages an add_messages reducer to append new messages rather than overwriting them, creating a memory of the interaction. The lesson then covered how to equip a chat model with tools by defining a standard Python function and binding it to the model using .bind_tools(). I applied these concepts by creating my own F1-themed example where I defined a decide_winner function to compare two drivers' points and bound it to a gpt-4.1-mini model. I then built a simple, single-node graph that calls this tool-enabled LLM. I confirmed my understanding by invoking the graph with different inputs: when I sent a simple greeting, the LLM responded conversationally, but when I asked, "Who is the world champion if Sebastian Vettel has 589 points and Lance Stroll has 12 points," the model correctly returned a tool_call for my decide_winner function with the appropriate arguments, demonstrating the core principle of an LLM-driven tool-use chain. (https://github.com/Nithurshen/nithurshen-langgraph-MAT496/blob/main/module-1/lesson_4/my_example_of_chain.ipynb)

![l4_m1](https://github.com/Nithurshen/nithurshen-langgraph-MAT496/blob/main/module-1/lesson_4/l4_m1.png)
* <b>Lesson 5:</b> I've learned how to build a router, which is a fundamental agentic pattern in LangGraph that can dynamically decide between responding directly or calling a tool. The key to this is adding a dedicated ToolNode to the graph to execute the function(s) and then using a conditional edge to direct the workflow. The lesson introduced the convenient pre-built tools_condition, which automatically checks if the last message from the LLM is a tool call and routes accordingly: to the ToolNode if a tool is invoked, or to the END if it's a standard response. I successfully applied this pattern by building my own F1-themed router with a custom check_winner tool. I constructed the graph with a ToolNode([check_winner]) and used tools_condition as the conditional edge. When I invoked the graph with a question about Max Verstappen's and Oscar Piastri's points, it correctly identified the need for the tool, routed the execution to the ToolNode, and returned a ToolMessage containing the result, 331. This demonstrates the core principle of a router: creating an agent that can choose its own path based on the user's input. (https://github.com/Nithurshen/nithurshen-langgraph-MAT496/blob/main/module-1/lesson_5/my_example_of_router.ipynb)

![l5_m1](https://github.com/Nithurshen/nithurshen-langgraph-MAT496/blob/main/module-1/lesson_5/l5_m1.png)
