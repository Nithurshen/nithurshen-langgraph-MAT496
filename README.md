# nithurshen-langgraph-MAT496
This repository documents my learning from the LangChain Academy's "Intro to LangGraph" course. Each commit corresponds to a specific video lesson and features personalized modifications to the original source code. The README file serves as a detailed log, containing a summary of key concepts from each video and notes on my code tweaks.

<b>Name:</b> K S Nithurshen  
<b>Roll No:</b> 2410110157  

## Module 0:
I've learned the fundamentals of setting up a LangChain environment and interacting with its core components. The course covered how to instantiate different ChatOpenAI models, like gpt-4o and gpt-3.5-turbo, and how to use the .invoke() method to send prompts. It also introduced the integration of external search tools, specifically using TavilySearchResults. I've put these concepts into practice in my own notebook by experimenting with other models like gpt-4.1-nano and gpt-4o-mini. To align the exercise with my interest in Formula 1, I modified the tool run by changing the TavilySearchResults query to "Who is Max Verstappen?" and increased the max_results parameter to 50 for a more extensive search. (https://github.com/Nithurshen/nithurshen-langgraph-MAT496/blob/main/module-0/my_example_of_basics.ipynb)

## Module 1: (Introduction)
* <b>Lesson 1:</b> I've learned about the fundamental trade-off in building LLM applications between the reliability of fixed, linear "chains" and the agency or flexibility of "agents". While chains are dependable because they execute the same control flow every time, they are limited. Agents are more powerful because they use an LLM to decide their own control flow, but this makes them less predictable. LangGraph is presented as the solution to this problem, designed to optimize the curve by enabling developers to build applications as graphs. This graph-based approach allows for a balance of reliability and control by letting the developer define the structure while injecting an LLM at specific points to make routing decisions. These graphs operate on a shared state or memory, and nodes can call tools to modify that state, providing a robust framework for creating more dependable and controllable agents.
* <b>Lesson 2:</b> Based on this lesson, I've learned how to build a simple application using LangGraph's core components. The process starts with defining a State using a TypedDict, which acts as a shared memory schema for the entire graph. I then created Nodes, which are Python functions that take this state as an argument and return a dictionary to update it. The key to creating dynamic workflows is using Conditional Edges, which are functions that route the execution to different nodes based on the current state. I put this into practice by building a more complex, F1-themed graph than the one in the lesson. My implementation uses eight nodes and three separate conditional edges to randomly generate a sentence about a race, deciding on the driver (e.g., Max Verstappen or Sebastian Vettel), the outcome (won or lost), and the specific event (Monaco GP or Le Mans). Finally, I compiled the graph and successfully used the .invoke() method to run it, confirming my understanding of how to construct and execute a stateful, branching workflow. (https://github.com/Nithurshen/nithurshen-langgraph-MAT496/blob/main/module-1/lesson_2/my_example_of_simple_graph.ipynb)

![l2_m1](https://github.com/Nithurshen/nithurshen-langgraph-MAT496/blob/main/module-1/lesson_2/l2_m1.png)
* <b>Lesson 3:</b> From this lesson, I've learned how LangSmith and the new LangGraph Studio are essential tools for debugging, visualizing, and iterating on LangGraph applications. The video explained that every time a graph, like the F1-themed one I wrote in f1_exmple.py, is run, it creates a detailed trace in LangSmith. As shown in the screenshot of my trace, this is incredibly powerful for debugging because it provides a step-by-step log of the execution flow and shows exactly how the Graph State changed as it moved through the nodes, from node_1 to node_2 ("I am Max Verstappen"), and eventually to node_8. The lesson also highlighted how the LangGraph Studio acts as a visual IDE. The left panel in my screenshot shows the complete structure of my graph, which was generated from my Python code, making it easy to understand the branching logic I implemented and how the final state was reached. (https://github.com/Nithurshen/nithurshen-langgraph-MAT496/blob/main/f1_exmple.py)

![l3_m1](https://github.com/Nithurshen/nithurshen-langgraph-MAT496/blob/main/module-1/lesson_3/l3_m1.png)
* <b>Lesson 4:</b> I've learned how to build a foundational LangGraph "chain" that can intelligently decide when to use external tools. A key concept was managing conversational history by using the pre-built MessagesState for the graph's state, which leverages an add_messages reducer to append new messages rather than overwriting them, creating a memory of the interaction. The lesson then covered how to equip a chat model with tools by defining a standard Python function and binding it to the model using .bind_tools(). I applied these concepts by creating my own F1-themed example where I defined a decide_winner function to compare two drivers' points and bound it to a gpt-4.1-mini model. I then built a simple, single-node graph that calls this tool-enabled LLM. I confirmed my understanding by invoking the graph with different inputs: when I sent a simple greeting, the LLM responded conversationally, but when I asked, "Who is the world champion if Sebastian Vettel has 589 points and Lance Stroll has 12 points," the model correctly returned a tool_call for my decide_winner function with the appropriate arguments, demonstrating the core principle of an LLM-driven tool-use chain. (https://github.com/Nithurshen/nithurshen-langgraph-MAT496/blob/main/module-1/lesson_4/my_example_of_chain.ipynb)

![l4_m1](https://github.com/Nithurshen/nithurshen-langgraph-MAT496/blob/main/module-1/lesson_4/l4_m1.png)
* <b>Lesson 5:</b> I've learned how to build a router, which is a fundamental agentic pattern in LangGraph that can dynamically decide between responding directly or calling a tool. The key to this is adding a dedicated ToolNode to the graph to execute the function(s) and then using a conditional edge to direct the workflow. The lesson introduced the convenient pre-built tools_condition, which automatically checks if the last message from the LLM is a tool call and routes accordingly: to the ToolNode if a tool is invoked, or to the END if it's a standard response. I successfully applied this pattern by building my own F1-themed router with a custom check_winner tool. I constructed the graph with a ToolNode([check_winner]) and used tools_condition as the conditional edge. When I invoked the graph with a question about Max Verstappen's and Oscar Piastri's points, it correctly identified the need for the tool, routed the execution to the ToolNode, and returned a ToolMessage containing the result, 331. This demonstrates the core principle of a router: creating an agent that can choose its own path based on the user's input. (https://github.com/Nithurshen/nithurshen-langgraph-MAT496/blob/main/module-1/lesson_5/my_example_of_router.ipynb)

![l5_m1](https://github.com/Nithurshen/nithurshen-langgraph-MAT496/blob/main/module-1/lesson_5/l5_m1.png)
* <b>Lesson 6:</b> I've learned how to advance from a simple router to a complete ReAct agent capable of multi-step reasoning and tool use. The key insight was extending the previous graph by adding a loop that feeds the output of a ToolNode back to the assistant node. This creates the core Reason-Act-Observe cycle, where the agent can call a tool (Act), receive the result (Observe), and then decide on the next step, whether it's calling another tool or generating a final response (Reason). I put this into practice by building an F1-themed agent with three custom tools: choose_winner, constructor_points, and percentage_better. I tested it with a complex query asking for the winner between two drivers, their combined constructor points, and the performance percentage difference. My agent successfully executed a three-step chain: it first called choose_winner, then used that result to call constructor_points, and finally percentage_better, before synthesizing all the tool outputs into a comprehensive final answer, demonstrating the power of this looped agentic architecture. (https://github.com/Nithurshen/nithurshen-langgraph-MAT496/blob/main/module-1/lesson_6/my_example_of_agent.ipynb)

![l6_m1_3](https://github.com/Nithurshen/nithurshen-langgraph-MAT496/blob/main/module-1/lesson_6/l6_m1_3.png)
* <b>Lesson 7:</b>  This lesson taught me how to give a ReAct agent memory to handle multi-turn conversations, overcoming the default stateless nature where context is lost after each invocation. I learned that the state is normally transient, which I first confirmed in my own F1-themed agent; when I asked a follow-up question about driver performance, the agent couldn't answer because it had forgotten the drivers' points from the previous turn. The solution presented was to use a checkpointer for persistence. By compiling my graph with an in-memory MemorySaver, the agent can automatically save its state after each step. To manage this, each conversation is assigned a unique thread_id within a config dictionary, which tells the graph where to save and retrieve the conversational state. When I applied this to my F1 agent and re-ran the conversation using the same thread_id, the agent successfully remembered the drivers' points from the first turn and used the correct tool (percentage_better) to answer my follow-up question, demonstrating the successful implementation of a stateful, memory-enabled agent. (https://github.com/Nithurshen/nithurshen-langgraph-MAT496/blob/main/module-1/lesson_7/my_example_of_agent_memory.ipynb)

## Module 2: (State and Memory)
* <b>Lesson 1:</b> I've learned how to define the state schema for a LangGraph graph using several different methods. The course covered the distinctions between using a TypedDict for simple type hinting, a dataclass for a structured data object, and a Pydantic BaseModel to enforce strict, runtime data validation. I've put these concepts into practice in my own notebook by creating a Formula 1-themed example graph that manages a driver's status and tyre compound. For the TypedDict and dataclass implementations, I defined a state with driver and tyre_compound keys. Finally, I successfully implemented a more robust version using a PydanticState model, adding a @field_validator to ensure the tyre_compound must be one of "Soft", "Medium", or "Hard", and confirmed it correctly raised a ValidationError when an invalid "Wet" compound was provided. (https://github.com/Nithurshen/nithurshen-langgraph-MAT496/blob/main/module_2/lesson_1/my_example_of_state_schema.ipynb)

![l1_m2](https://github.com/Nithurshen/nithurshen-langgraph-MAT496/blob/main/module_2/lesson_1/l1_m2.png)
* <b>Lesson 2:</b> I've learned about state reducers in LangGraph and how they define the way state updates are handled. The course explained that the default behavior is to overwrite state values, which can lead to an InvalidUpdateError if multiple nodes running in parallel try to update the same state key in a single step. I practiced solving this by using the Annotated type hint with the built-in operator.add reducer in my own F1-themed notebook. This allowed me to correctly append lap times to a list when multiple nodes recorded times concurrently. I also encountered the TypeError that occurs when using add if the initial state is None, and implemented a custom reducer function (reduce_lap_times_list) to handle these None cases gracefully, ensuring the list concatenation still worked. Additionally, the lesson covered the built-in add_messages reducer, showing how it can not only append messages but also overwrite them based on message ID and remove messages using RemoveMessage, which I experimented with in my F1 team radio examples. (https://github.com/Nithurshen/nithurshen-langgraph-MAT496/blob/main/module_2/lesson_2/my_example_of_state_reducers.ipynb)

![l2_m2](https://github.com/Nithurshen/nithurshen-langgraph-MAT496/blob/main/module_2/lesson_2/l2_m2.png)
* <b>Lesson 3:</b> I've learned how to manage multiple state schemas within a single LangGraph graph. The course demonstrated two main techniques: using private state for intermediate data and defining distinct input/output schemas for the overall graph. I practiced the private state concept by creating an F1 example where a race_control_node accepted one state type (RaceControlState) and returned a different one (TeamStrategyState) containing intermediate data, which was then consumed by the team_strategy_node before returning the final RaceControlState. This showed how nodes can communicate using different data structures internally. Furthermore, I implemented separate input and output schemas by defining RaceInputState, RaceOutputState, and FullRaceStateInternal for my race results graph. By initializing the StateGraph with input_schema=RaceInputState and output_schema=RaceOutputState, I successfully constrained the graph to accept only the driver_name as input and return only the final_position as output, even though internal nodes like timing_node calculated and used additional fields like fastest_lap. The lesson also emphasized using type hints on node functions to clearly specify which schema they expect as input or will return as output, which helps maintain clarity when working with multiple schemas. (https://github.com/Nithurshen/nithurshen-langgraph-MAT496/blob/main/module_2/lesson_3/my_example_of_multiple_schemas.ipynb)

![l3_m2](https://github.com/Nithurshen/nithurshen-langgraph-MAT496/blob/main/module_2/lesson_3/l3_m2.png)
* <b>Lesson 4:</b> I've learned effective techniques for managing message history within LangGraph state, specifically focusing on filtering and trimming messages to handle long conversations efficiently. The course explained two main approaches: filtering, where you select a subset of messages to pass to a node (like an LLM) without altering the graph's overall message state, and trimming, which uses the trim_messages utility to limit the messages passed to a node based on token count. I practiced filtering in my F1 team radio notebook by modifying the node to only pass the latest message to the LLM using list slicing (state["messages"][-1:]). For trimming, I implemented a node that used trim_messages with max_tokens=100 and strategy="last" to limit the context sent to the model, observing how it affected the conversation history passed during invocation. Additionally, the lesson revisited using the add_messages reducer with RemoveMessage as a way to permanently filter messages from the state itself, which I demonstrated in an F1 example node (filter_radio_messages) designed to keep only the last two messages in the state. (https://github.com/Nithurshen/nithurshen-langgraph-MAT496/blob/main/module_2/lesson_4/my_example_of_trim_filter_messages.ipynb)

![l4_m2_2](https://github.com/Nithurshen/nithurshen-langgraph-MAT496/blob/main/module_2/lesson_4/l4_m2_2.png)
* <b>Lesson 5:</b> I've learned how to build a chatbot with memory and conversation summarization using LangGraph, moving beyond simple message filtering/trimming. The course demonstrated using an LLM to create a running summary of the conversation, which is stored in a custom state key (summary or race_summary) alongside the messages. I put this into practice in my F1-themed notebook by creating a RaceSessionState inheriting from MessagesState and adding a race_summary field. I implemented a node (call_race_control) that includes the current summary as a SystemMessage when calling the LLM, and another node (generate_race_report) that uses the LLM to update this summary based on recent messages. A conditional edge (should_generate_report) was used to trigger the summarization node only when the message count exceeded a threshold (e.g., > 6 messages). After generating the summary, the generate_race_report node used RemoveMessage to filter the main message list, keeping only the latest messages (last 2) to manage context length effectively. Finally, I successfully implemented memory by compiling the graph with a MemorySaver checkpointer and used a thread_id in the config object to persist and retrieve the conversation state (including the summary and filtered messages) across multiple invoke calls for the "race_1" session. (https://github.com/Nithurshen/nithurshen-langgraph-MAT496/blob/main/module_2/lesson_5/my_example_of_chatbot_summarization.ipynb)

![l5_m2](https://github.com/Nithurshen/nithurshen-langgraph-MAT496/blob/main/module_2/lesson_5/l5_m2.png)
* <b>Lesson 6:</b> I learned how to implement indefinite persistence for chatbot memory using external databases, specifically using the SqliteSaver checkpointer. This allows conversation state to persist even after the application restarts, unlike the in-memory MemorySaver used previously. I applied this by connecting to a local SQLite database (f1_race_data.db) and passing the SqliteSaver instance during graph compilation. To manage potentially long conversation histories within this persistent memory, I adapted the lesson's summarization technique for my F1 race context. I created a custom RaceSessionState inheriting from MessagesState to include a race_summary field. My graph uses a call_race_control node that incorporates the existing summary into the LLM prompt if available, and a generate_race_report node triggered by the should_generate_report conditional edge after 6 messages. This generate_race_report node updates the summary and uses RemoveMessage to prune older messages, keeping the state manageable while retaining context. As shown in the LangSmith trace, the chatbot successfully uses the stored Race Summary from the database to answer follow-up questions, demonstrating effective long-term, persistent memory management. (https://github.com/Nithurshen/nithurshen-langgraph-MAT496/blob/main/module_2/lesson_6/my_example_of_chatbot_external_memory.ipynb), (https://github.com/Nithurshen/nithurshen-langgraph-MAT496/blob/main/module_2/lesson_6/f1_race_data.py)

![l6_m2](https://github.com/Nithurshen/nithurshen-langgraph-MAT496/blob/main/module_2/lesson_6/l6_m2.png)
## Module 3: (UX and Human-in-the-Loop)
* <b>Lesson 1:</b> I've learned how to stream outputs from a LangGraph application in various ways. The process starts with streaming the full graph state using the .stream() method with stream_mode='values', which returns the complete state after each node is called. I also learned to stream just the changes using stream_mode='updates', which provides only the dictionary of updates to the state from the last node run. The key to creating a real-time chat experience is using the .astream_events() method, which allows me to listen for specific events like on_chat_model_stream and access the token chunks as they are generated. I put this into practice by building my own F1-themed graph with a RaceSessionState and a radio_comms node. My implementation tested all the local streaming methods (values, updates, and astream_events) on this custom graph to monitor its execution and token output. Finally, I connected to the LangGraph API client and successfully used client.runs.stream() with the API-only messages stream mode to stream from my deployed f1_race_data assistant, confirming my understanding of both local and remote streaming workflows. (https://github.com/Nithurshen/nithurshen-langgraph-MAT496/blob/main/module_3/lesson_1/my_example_of_streaming_interruption.ipynb)
* <b>Lesson 2:</b> I've learned how to stream outputs from a LangGraph application in various ways. The process starts with streaming the full graph state using the `.stream()` method with `stream_mode='values'`, which returns the complete state after each node is called. I also learned to stream just the changes using `stream_mode='updates'`, which provides only the dictionary of updates to the state from the last node run. The key to creating a real-time chat experience is using the `.astream_events()` method, which allows me to listen for specific events like `on_chat_model_stream` and access the token chunks as they are generated. I put this into practice by building my own F1-themed graph with a `RaceSessionState` and a `radio_comms` node. My implementation tested all the local streaming methods (`values`, `updates`, and `astream_events`) on this custom graph to monitor its execution and token output. Finally, I connected to the LangGraph API client and successfully used `client.runs.stream()` with the API-only `messages` stream mode to stream from my deployed `f1_race_data` assistant, confirming my understanding of both local and remote streaming workflows. (https://github.com/Nithurshen/nithurshen-langgraph-MAT496/blob/main/module_3/lesson_2/my_example_of_breakpoints.ipynb), (https://github.com/Nithurshen/nithurshen-langgraph-MAT496/blob/main/module_3/lesson_2/f1_race_data.py)

![l2_m3](https://github.com/Nithurshen/nithurshen-langgraph-MAT496/blob/main/module_3/lesson_2/l2_m3.png)
* <b>Lesson 3:</b> I've learned how to interrupt a LangGraph execution and edit its state, enabling human-in-the-loop workflows like debugging or providing feedback. The process starts with compiling the graph using interrupt_before=['node_name'] to pause execution right before a specific node runs. I also learned that once interrupted, I can inspect the current state using graph.get_state(thread) and modify it using graph.update_state(thread, updates), which leverages the state's reducer logic (like appending messages for MessagesState or overwriting if a message id is provided). The key to incorporating explicit human feedback is adding a placeholder node (like human_feedback) and interrupting before it, then using graph.update_state(thread, feedback, as_node='human_feedback') to apply the user's input as if it came from that node. I put this into practice by building an F1 Race Engineer agent with tools to check tyre wear, get weather forecasts, and suggest pit strategies. My implementation first demonstrated local state editing: I interrupted the graph before the assistant node, used get_state to view the messages, update_state to add a clarifying human message ("Actually, check wear for lap 12."), and then resumed execution by calling stream(None, thread). Finally, I connected to the LangGraph API client, ran my deployed f1_race_data assistant, interrupted it remotely by passing interrupt_before=['assistant'] to client.runs.stream, fetched the state with client.threads.get_state, directly modified the content of the last human message in the retrieved state object, pushed the change back using client.threads.update_state (overwriting the original message because the ID was included), and resumed the run by streaming again with input=None, confirming my understanding of how to manage and modify state both locally and via the API. (https://github.com/Nithurshen/nithurshen-langgraph-MAT496/blob/main/module_3/lesson_3/my_example_of_edit_state_human_feedback.ipynb)

![l3_m3](https://github.com/Nithurshen/nithurshen-langgraph-MAT496/blob/main/module_3/lesson_3/l3_m3.png)
* <b>Lesson 4:</b> Based on these lessons, I've learned how to stream outputs from LangGraph using .stream() with 'values' or 'updates' modes, .astream_events() for real-time token streaming, and the API's client.runs.stream() with 'values' and 'messages' modes, practicing these techniques on a custom F1-themed graph both locally and via the API client. Building on this, I learned to interrupt graph execution using interrupt_before, inspect the state with get_state, modify it via update_state (leveraging state reducer logic and optionally providing message IDs for overwrites), and incorporate human feedback using placeholder nodes and update_state(..., as_node='...'); I applied this to an F1 Race Engineer agent, successfully editing messages during pauses both locally and through the API. Lastly, I explored dynamic breakpoints by raising NodeInterrupt within a node based on runtime conditions (like urgent F1 driver messages), understanding that this requires a state update via update_state to proceed past the interruption, and validated this process on another F1 agent locally and via the API, visualizing the interrupts in LangSmith Studio. (https://github.com/Nithurshen/nithurshen-langgraph-MAT496/blob/main/module_3/lesson_4/my_example_of_dynamic_breakpoints.ipynb), (https://github.com/Nithurshen/nithurshen-langgraph-MAT496/blob/main/module_3/lesson_4/f1_dynamic_breakpoints.py)

![l4_m3](https://github.com/Nithurshen/nithurshen-langgraph-MAT496/blob/main/module_3/lesson_4/l4_m3.png)
* <b>Lesson 5:</b> I've learned how to leverage LangGraph's "time travel" capabilities for debugging and exploring alternative execution paths. The process starts with accessing the execution history using graph.get_state_history(config) to retrieve a list of past StateSnapshot objects. I then learned how to replay execution from a specific point in the past by extracting the config (containing the checkpoint_id) from a chosen StateSnapshot and passing it back into graph.stream(None, config). The key to exploring alternatives is forking: this involves selecting a past state's config, using graph.update_state(config, updates) to modify the state (like changing a message, often using its ID to overwrite), which creates a new configuration (fork_config) representing the branched state, and then running the graph from this new config using graph.stream(None, fork_config). I put this into practice using my F1 Race Engineer agent (f1_race_data.py). In the notebook (my_example_of_time_travel.ipynb), I first ran the agent normally, then retrieved its history. I selected a state from the history (all_states[-2]), demonstrated replaying from it by streaming with its original config, and then demonstrated forking by creating a modified state (fork_config) using update_state on the same past config (specifically changing the lap number in the initial query) and streaming from this new config to observe the different outcome. I repeated these replay and fork operations using the LangGraph API client, employing client.threads.get_history, client.runs.stream(..., checkpoint_id=...), and client.threads.update_state(..., checkpoint_id=...) to interact with the deployed agent, visualizing the process in LangSmith Studio. Finally, this confirmed my understanding of how to navigate, replay, and fork execution histories for debugging and exploring different scenarios within a LangGraph application. (https://github.com/Nithurshen/nithurshen-langgraph-MAT496/blob/main/module_3/lesson_5/my_example_of_time_travel.ipynb)

![l5_m3](https://github.com/Nithurshen/nithurshen-langgraph-MAT496/blob/main/module_3/lesson_5/l5_m3.png)

## Module 4: (Building your Assistant)
* <b>Lesson 1:</b> I've learned how to manage parallel node execution within a single LangGraph graph. The course demonstrated two main concepts: handling concurrent state updates and synchronizing parallel branches of different lengths. I practiced the concurrent update concept by creating my own custom F1 race example where a graph fanned out from a lights_out node to parallel team_verstappen and team_leclerc nodes. This correctly reproduced the InvalidUpdateError from the lesson, which I then solved by applying Annotated[list, operator.add] to my RaceState's race_report key. This showed how the reducer function works to concatenate the outputs from both parallel nodes into a list rather than allowing them to overwrite each other. Furthermore, I implemented branch synchronization by building an asymmetric version of my F1 graph, with one branch team_verstappen -> verstappen_outlap and a shorter parallel branch team_leclerc. By defining the fan-in edge to the final chequered_flag node as ["verstappen_outlap", "team_leclerc"], I successfully demonstrated that the graph waits for all branches to complete before proceeding. This concept was then applied in a practical example (as shown in the diagram) where get_driver_stats and get_race_telemetry nodes run in parallel to populate a context field for a final generate_race_summary node. The lesson also emphasized using custom reducer functions, like the sorting_reducer I implemented, to explicitly control the order of state updates when parallel branches are combined.